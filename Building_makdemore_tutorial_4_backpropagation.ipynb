{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5ba231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079bf990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ccbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b4808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "  \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6636f9",
   "metadata": {},
   "source": [
    "## above is biolerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5e4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function \n",
    "# to compare with manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470b63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "# the dimensionality of the character embedding vectors\n",
    "n_embd = 10 \n",
    "# the number of neurons in the hidden layer of the MLP\n",
    "n_hidden = 64 \n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "# b1 is useless here because of BN\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 \n",
    "\n",
    "\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# initializating many of these parameters in non-standard ways to avoid trouble later (e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass).\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "\n",
    "# number of parameters in total\n",
    "print(sum(p.nelement() for p in parameters)) \n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22003fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3441, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "# embedding\n",
    "emb = C[Xb] \n",
    "# concatenate the vectors\n",
    "embcat = emb.view(emb.shape[0], -1) \n",
    "\n",
    "# Linear layer 1\n",
    "# hidden layer pre-activation\n",
    "hprebn = embcat @ W1 + b1 \n",
    "\n",
    "\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "\n",
    "# Non-linearity\n",
    "# still part of the hidden layer\n",
    "h = torch.tanh(hpreact) \n",
    "\n",
    "\n",
    "# Linear layer 2\n",
    "# output layer\n",
    "logits = h @ W2 + b2 \n",
    "\n",
    "# spelling out the cross entropy loss\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "# subtract max for numerical stability\n",
    "norm_logits = logits - logit_maxes \n",
    "\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64189c28",
   "metadata": {},
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8265290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogprobs.shape\n",
    "# dlogprobs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47fc1919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a349ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogprobs[range(n), Yb].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625add68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d897599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4d74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "# dcounts = counts_sum_inv * dprobs\n",
    "# dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# dnorm_logits = counts * dcounts\n",
    "# dlogits = dnorm_logits.clone()\n",
    "# dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ef3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dh, dh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f8909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "\n",
    "dh = dlogits @ W2.T  #make the shapes work out\n",
    "\n",
    "# da * dd\n",
    "# dlogits and multiply\n",
    "\n",
    "# dlogits\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3794fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53d833",
   "metadata": {},
   "source": [
    "## a11, a12\n",
    "## a21, a22\n",
    "#### ---> b1  b2, where:\n",
    "#### b1 = 1/(n-1)*(a11 + a21)\n",
    "#### b2 = 1/(n-1)((a12 + a22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cd02058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667483dc",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14828e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d2bbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3441264629364014 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f1cb277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e31ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe7640aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0717, 0.0750, 0.0176, 0.0534, 0.0196, 0.0865, 0.0251, 0.0384, 0.0187,\n",
       "         0.0306, 0.0366, 0.0333, 0.0397, 0.0287, 0.0327, 0.0134, 0.0093, 0.0187,\n",
       "         0.0161, 0.0581, 0.0506, 0.0239, 0.0268, 0.0668, 0.0623, 0.0251, 0.0214],\n",
       "        [0.0550, 0.0516, 0.0944, 0.0551, 0.0352, 0.0299, 0.0202, 0.0494, 0.0220,\n",
       "         0.0250, 0.0515, 0.0412, 0.0475, 0.0272, 0.0448, 0.0395, 0.0257, 0.0171,\n",
       "         0.0208, 0.0375, 0.0187, 0.0257, 0.0157, 0.0628, 0.0245, 0.0363, 0.0259]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b8b7ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0717,  0.0750,  0.0176,  0.0534,  0.0196,  0.0865,  0.0251,  0.0384,\n",
       "        -0.9813,  0.0306,  0.0366,  0.0333,  0.0397,  0.0287,  0.0327,  0.0134,\n",
       "         0.0093,  0.0187,  0.0161,  0.0581,  0.0506,  0.0239,  0.0268,  0.0668,\n",
       "         0.0623,  0.0251,  0.0214], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1675c543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.7940e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64e45ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x142dbd890>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49a4cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f55949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e32a4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a59c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
